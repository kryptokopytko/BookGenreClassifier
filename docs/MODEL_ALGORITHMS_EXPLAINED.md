# ğŸ“š Przewodnik po Algorytmach Klasyfikacji - Jak DziaÅ‚ajÄ… Modele

---

## ğŸ“– Spis TreÅ›ci

1. [Wprowadzenie](#wprowadzenie)
2. [Modele Oparte na TekÅ›cie (TF-IDF)](#modele-oparte-na-tekÅ›cie-tf-idf)
   - Linear SVM
   - Logistic Regression
   - Naive Bayes
   - Ridge Classifier
   - K-Nearest Neighbors (KNN)
   - Nearest Centroid
3. [Modele Oparte na Cechach](#modele-oparte-na-cechach)
   - Random Forest
   - XGBoost
   - LightGBM
4. [Modele Hybrydowe](#modele-hybrydowe)
   - Style-based Model
   - Baseline Keyword Model
5. [Ensemble Models](#ensemble-models)
6. [PorÃ³wnanie AlgorytmÃ³w](#porÃ³wnanie-algorytmÃ³w)
7. [Kiedy KtÃ³rego UÅ¼yÄ‡](#kiedy-ktÃ³rego-uÅ¼yÄ‡)

---

## Wprowadzenie

W tym projekcie uÅ¼ywamy rÃ³Å¼nych algorytmÃ³w uczenia maszynowego do klasyfikacji gatunkÃ³w ksiÄ…Å¼ek. KaÅ¼dy algorytm ma inne podejÅ›cie do problemu i dziaÅ‚a lepiej w rÃ³Å¼nych sytuacjach.

### Podstawowe PojÄ™cia

**TF-IDF (Term Frequency-Inverse Document Frequency)**

- SposÃ³b reprezentacji tekstu jako liczb
- **TF**: Jak czÄ™sto sÅ‚owo pojawia siÄ™ w dokumencie
- **IDF**: Jak rzadkie jest sÅ‚owo w caÅ‚ym korpusie
- WaÅ¼ne sÅ‚owa majÄ… wysokie wartoÅ›ci, czÄ™ste sÅ‚owa ("the", "is") niskie

**Feature Engineering**

- WyciÄ…ganie uÅ¼ytecznych informacji z danych
- Np. liczba sÅ‚Ã³w, Å›rednia dÅ‚ugoÅ›Ä‡ zdania, stosunek dialogÃ³w

**Overfitting**

- Model "nauczyÅ‚ siÄ™ na pamiÄ™Ä‡" dane treningowe
- DziaÅ‚a Å›wietnie na treningu, Åºle na testach
- RozwiÄ…zanie: regularyzacja, mniej parametrÃ³w

---

## Modele Oparte na TekÅ›cie (TF-IDF)

### 1. ğŸ¯ Linear SVM (Support Vector Machine)

**Jak dziaÅ‚a:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 1: Reprezentuj teksty jako    â”‚
â”‚  wektory TF-IDF (15000 wymiarÃ³w)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 2: ZnajdÅº hiperpÅ‚aszczyznÄ™    â”‚
â”‚  ktÃ³ra najlepiej separuje klasy     â”‚
â”‚                                      â”‚
â”‚    Romance  â”‚  Mystery               â”‚
â”‚      â€¢      â”‚    â€¢                   â”‚
â”‚    â€¢  â€¢     â”‚  â€¢   â€¢                â”‚
â”‚      â€¢    [GRANICA]  â€¢              â”‚
â”‚              â”‚    â€¢                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 3: Nowy tekst â†’ po ktÃ³rej     â”‚
â”‚  stronie granicy leÅ¼y?              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Matematyka (uproszczona):**

- SVM szuka granicy (hiperpÅ‚aszczyzny) ktÃ³ra maksymalizuje margines miÄ™dzy klasami
- Dla punktu x, decyzja: `sign(wÂ·x + b)`
- `w` = wektor wag (nauczony)
- `b` = bias (nauczony)

**Zalety:**

- âœ… Åšwietnie dziaÅ‚a na wysokowymiarowych danych (tekst)
- âœ… Odporny na overfitting
- âœ… Szybkie predykcje
- âœ… Teoretyczne podstawy (maksymalizacja marginesu)

**Wady:**

- âŒ DÅ‚ugi czas treningu dla duÅ¼ych zbiorÃ³w
- âŒ Trudno interpretowaÄ‡ (nie wiadomo "dlaczego")
- âŒ Potrzebuje dobrego skalowania danych

**Kiedy uÅ¼yÄ‡:**

- Dane tekstowe z TF-IDF
- ZaleÅ¼y Ci na accuracy
- Masz wiÄ™cej cech niÅ¼ sampli

**W naszym projekcie:**

- **Accuracy: 72.6%** (najlepszy model!)
- UÅ¼ywa 15000 cech TF-IDF
- Kernel: liniowy (najszybszy dla tekstu)

---

### 2. ğŸ“Š Logistic Regression

**Jak dziaÅ‚a:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 1: TF-IDF representation      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 2: Dla kaÅ¼dej klasy,         â”‚
â”‚  oblicz prawdopodobieÅ„stwo:         â”‚
â”‚                                      â”‚
â”‚  P(Romance|text) = Ïƒ(wâ‚Â·x + bâ‚)    â”‚
â”‚  P(Mystery|text) = Ïƒ(wâ‚‚Â·x + bâ‚‚)    â”‚
â”‚  ...                                â”‚
â”‚                                      â”‚
â”‚  Ïƒ = sigmoid (0-1)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 3: Wybierz klasÄ™ z highest   â”‚
â”‚  prawdopodobieÅ„stwem                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Matematyka:**

- Sigmoid function: `Ïƒ(z) = 1/(1 + e^(-z))`
- Dla multi-class: softmax
- `P(class_i|x) = exp(w_iÂ·x) / Î£ exp(w_jÂ·x)`

**Zalety:**

- âœ… Daje prawdopodobieÅ„stwa (nie tylko klasy)
- âœ… Szybki trening i predykcja
- âœ… Åatwo interpretowaÄ‡ wagi
- âœ… Regularizacja (L1/L2) zapobiega overfittingowi

**Wady:**

- âŒ ZakÅ‚ada liniowÄ… separowalnoÅ›Ä‡
- âŒ MoÅ¼e byÄ‡ zbyt prosty dla zÅ‚oÅ¼onych wzorcÃ³w

**Kiedy uÅ¼yÄ‡:**

- Potrzebujesz prawdopodobieÅ„stw
- Chcesz zrozumieÄ‡, ktÃ³re sÅ‚owa sÄ… waÅ¼ne
- Baseline model (zawsze zacznij od tego!)

**W naszym projekcie:**

- **Accuracy: 65.5%**
- Regularizacja: C=2.0 (mniej restrykcyjna)
- Solver: SAGA (dobry dla duÅ¼ych danych)

---

### 3. ğŸ² Naive Bayes (MultinomialNB)

**Jak dziaÅ‚a:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bayes Theorem:                     â”‚
â”‚  P(Genre|Words) =                   â”‚
â”‚    P(Words|Genre) Ã— P(Genre)        â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚
â”‚         P(Words)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "Naive" assumption:                â”‚
â”‚  SÅ‚owa sÄ… niezaleÅ¼ne!               â”‚
â”‚                                      â”‚
â”‚  P(wâ‚,wâ‚‚,...|Genre) =              â”‚
â”‚    P(wâ‚|Genre) Ã— P(wâ‚‚|Genre) Ã— ... â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dla nowego tekstu:                 â”‚
â”‚  Oblicz P(Genre|words) dla kaÅ¼dego â”‚
â”‚  gatunku â†’ wybierz max             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Intuicja:**

- Model "pamiÄ™ta" jakie sÅ‚owa wystÄ™pujÄ… w kaÅ¼dym gatunku
- Np. w Romance: "love"=czÄ™sto, "murder"=rzadko
- Nowy tekst z "love" â†’ prawdopodobnie Romance

**Zalety:**

- âœ… BARDZO szybki (zarÃ³wno trening jak predykcja)
- âœ… DziaÅ‚a dobrze na maÅ‚ych zbiorach
- âœ… Prosty i interpretowalny
- âœ… Naturalnie obsÅ‚uguje multi-class

**Wady:**

- âŒ Naiwne zaÅ‚oÅ¼enie (sÅ‚owa NIE sÄ… niezaleÅ¼ne!)
- âŒ WraÅ¼liwy na dane spoza treningu (smoothing pomaga)
- âŒ Nie uczy siÄ™ interakcji miÄ™dzy sÅ‚owami

**Kiedy uÅ¼yÄ‡:**

- MaÅ‚y zbiÃ³r danych
- Potrzebujesz SZYBKOÅšCI
- Baseline model
- Spam detection, sentiment analysis

**W naszym projekcie:**

- **Accuracy: 59.4%**
- Alpha=1.0 (Laplace smoothing)
- Dobry jako szybki baseline

---

### 4. ğŸ“ Ridge Classifier

**Jak dziaÅ‚a:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  To Logistic Regression ale z       â”‚
â”‚  regularizacjÄ… L2 (Ridge)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Minimize: Loss + Î± Ã— ||w||Â²       â”‚
â”‚                                      â”‚
â”‚  ||w||Â² = suma kwadratÃ³w wag       â”‚
â”‚  Î± = siÅ‚a regularyzacji            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Efekt: MaÅ‚e wagi â†’ mniej          â”‚
â”‚  overfittingu â†’ lepsze generalizationâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Matematyka:**

- Loss function: `L = (y - Å·)Â² + Î±Â·Î£wÂ²`
- Regularyzacja "karze" duÅ¼e wagi
- Î±=0 â†’ no regularization
- Î±=âˆ â†’ wszystkie wagi â†’ 0

**Zalety:**

- âœ… Bardzo odporny na overfitting
- âœ… DziaÅ‚a gdy features > samples
- âœ… Stabilny numerycznie
- âœ… Szybki

**Wady:**

- âŒ Nie robi feature selection (wszystkie features majÄ… wagi)
- âŒ Mniej elastyczny niÅ¼ modele nieliniowe

**Kiedy uÅ¼yÄ‡:**

- DuÅ¼o cech (high-dimensional)
- Problem z overfittingiem
- Stabilne predykcje waÅ¼niejsze niÅ¼ max accuracy

**W naszym projekcie:**

- Alpha=1.0 (standardowa regularizacja)
- Dobra alternatywa dla Logistic Regression

---

### 5. ğŸ‘¥ K-Nearest Neighbors (KNN)

**Jak dziaÅ‚a:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 1: Zapisz wszystkie dane     â”‚
â”‚  treningowe (no training!)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 2: Nowy tekst â†’ znajdÅº K     â”‚
â”‚  najbliÅ¼szych sÄ…siadÃ³w              â”‚
â”‚                                      â”‚
â”‚      ?                               â”‚
â”‚     / \                             â”‚
â”‚    â€¢1 â€¢2  (K=3)                    â”‚
â”‚      â€¢3                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 3: GÅ‚osowanie                 â”‚
â”‚  SÄ…siad 1: Romance                  â”‚
â”‚  SÄ…siad 2: Romance                  â”‚
â”‚  SÄ…siad 3: Mystery                  â”‚
â”‚  â†’ Predykcja: Romance (2/3)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Metryki odlegÅ‚oÅ›ci:**

- **Cosine similarity** (uÅ¼ywamy): kÄ…t miÄ™dzy wektorami
  - Dobra dla tekstu (niezaleÅ¼na od dÅ‚ugoÅ›ci)
  - `similarity = (AÂ·B)/(||A||Ã—||B||)`
- **Euclidean**: zwykÅ‚a odlegÅ‚oÅ›Ä‡
- **Manhattan**: suma rÃ³Å¼nic bezwzglÄ™dnych

**Zalety:**

- âœ… Prosty koncepcyjnie
- âœ… Brak fazy treningu
- âœ… MoÅ¼e uchwyciÄ‡ zÅ‚oÅ¼one granice decyzyjne
- âœ… Naturalnie multi-class

**Wady:**

- âŒ WOLNE predykcje (musi porÃ³wnaÄ‡ ze wszystkimi samples)
- âŒ WraÅ¼liwy na irrelevant features
- âŒ Potrzebuje duÅ¼o pamiÄ™ci
- âŒ Curse of dimensionality

**Kiedy uÅ¼yÄ‡:**

- MaÅ‚y dataset
- Nieregularne granice klas
- Nie ma czasu na tuning
- Chcesz "explainable" predictions (pokaÅ¼ sÄ…siadÃ³w)

**W naszym projekcie:**

- K=20 neighbors
- Metric: cosine
- Weights: distance (bliÅ¼si waÅ¼niejsi)

---

### 6. ğŸ¯ Nearest Centroid

**Jak dziaÅ‚a:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 1: Oblicz centroid (Å›redniÄ…)  â”‚
â”‚  dla kaÅ¼dej klasy                   â”‚
â”‚                                      â”‚
â”‚  Centroid_Romance = Å›rednia wszystkichâ”‚
â”‚                     tekstÃ³w Romance  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 2: Reprezentuj centroids      â”‚
â”‚  jako wektory TF-IDF                â”‚
â”‚                                      â”‚
â”‚    Câ‚ (Romance)                     â”‚
â”‚       â˜…                             â”‚
â”‚           â˜… Câ‚‚ (Mystery)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 3: Nowy tekst â†’ ktÃ³ry        â”‚
â”‚  centroid jest najbliÅ¼ej?           â”‚
â”‚                                      â”‚
â”‚    Câ‚        ?        Câ‚‚           â”‚
â”‚     â˜…        â€¢         â˜…           â”‚
â”‚        dâ‚ < dâ‚‚                     â”‚
â”‚    â†’ Predykcja: Câ‚ (Romance)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Intuicja:**

- KaÅ¼dy gatunek ma "prototypowy" dokument (centroid)
- Nowy tekst â†’ ktÃ³ry prototyp jest najbardziej podobny?

**Zalety:**

- âœ… BARDZO szybkie predykcje (tylko N porÃ³wnaÅ„, nie NÃ—samples)
- âœ… MaÅ‚o pamiÄ™ci (tylko centroids)
- âœ… Prosty i interpretowalny
- âœ… MoÅ¼na zobaczyÄ‡ "typowe sÅ‚owa" dla kaÅ¼dego gatunku

**Wady:**

- âŒ ZakÅ‚ada, Å¼e klasy sÄ… "kuliste" (convex)
- âŒ WraÅ¼liwy na outliery
- âŒ Nie uczy siÄ™ zÅ‚oÅ¼onych granic

**Kiedy uÅ¼yÄ‡:**

- Potrzebujesz SZYBKOÅšCI (production)
- MaÅ‚o pamiÄ™ci
- Klasy sÄ… dobrze separowane
- Chcesz zrozumieÄ‡ "typowy" dokument kaÅ¼dej klasy

**W naszym projekcie:**

- Metric: euclidean (cosine nie dziaÅ‚a w sklearn)
- Shrink_threshold: None (no shrinkage)

---

## Modele Oparte na Cechach

### 7. ğŸŒ³ Random Forest

**Jak dziaÅ‚a:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 1: Zbuduj wiele drzew        â”‚
â”‚  decyzyjnych (forest)               â”‚
â”‚                                      â”‚
â”‚  Drzewo 1      Drzewo 2   Drzewo N â”‚
â”‚     ğŸŒ²           ğŸŒ²          ğŸŒ²     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 2: KaÅ¼de drzewo gÅ‚osuje      â”‚
â”‚                                      â”‚
â”‚  Tree 1: Romance                    â”‚
â”‚  Tree 2: Romance                    â”‚
â”‚  Tree 3: Mystery                    â”‚
â”‚  ...                                â”‚
â”‚  Tree 200: Romance                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 3: Majority vote             â”‚
â”‚  â†’ Final prediction: Romance        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Jak budowaÄ‡ drzewa:**

```
                Root
                 â”‚
         [avg_word_len > 5.2?]
               /    \
             Yes     No
             â”‚       â”‚
      [dialogue_ratio>0.3?]  [Chapter count>10?]
         /    \               /    \
     Romance Mystery      SciFi  History
```

**Randomizacja:**

1. **Bagging**: KaÅ¼de drzewo trenuje na losowym subsecie danych
2. **Feature sampling**: KaÅ¼dy split patrzy na losowy subset cech

**Zalety:**

- âœ… Bardzo mocny (czÄ™sto top performance)
- âœ… ObsÅ‚uguje nieliniowe zaleÅ¼noÅ›ci
- âœ… Feature importance (ktÃ³re cechy waÅ¼ne)
- âœ… Nie potrzebuje skalowania
- âœ… Odporny na outliery

**Wady:**

- âŒ MoÅ¼e overfittowaÄ‡ (jak w naszym projekcie!)
- âŒ Wolniejszy niÅ¼ linear models
- âŒ Trudniej interpretowaÄ‡
- âŒ DuÅ¼y rozmiar modelu

**Jak zapobiec overfittingowi:**

```python
# PRZED (overfitting)
max_depth=15          # Zbyt gÅ‚Ä™bokie drzewa
min_samples_split=10  # Zbyt maÅ‚e

# PO (lepsze)
max_depth=8           # PÅ‚ytsze drzewa
min_samples_split=20  # WiÄ™cej sampli na split
min_samples_leaf=10   # Min sampli w liÅ›ciu
max_features='sqrt'   # Mniej cech na split
n_estimators=200      # WiÄ™cej drzew
```

**Kiedy uÅ¼yÄ‡:**

- Masz duÅ¼o rÃ³Å¼nych typÃ³w cech
- Nieliniowe zaleÅ¼noÅ›ci
- Feature importance jest waÅ¼na
- MoÅ¼esz poÅ›wiÄ™ciÄ‡ trochÄ™ czasu na trening

**W naszym projekcie:**

- **OLD: 100% train â†’ 55.8% test** (OVERFITTING!)
- **NEW (po fix): spodziewamy siÄ™ ~65-70%**
- 200 drzew, max_depth=8

---

### 8. ğŸš€ XGBoost (eXtreme Gradient Boosting)

**Jak dziaÅ‚a:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 1: Zbuduj drzewo #1          â”‚
â”‚  (prÃ³buje predyktowaÄ‡ labels)       â”‚
â”‚                                      â”‚
â”‚     Predictionsâ‚ = [0.3, 0.7, ...]  â”‚
â”‚     Errorsâ‚ = y - Predictionsâ‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 2: Zbuduj drzewo #2          â”‚
â”‚  (prÃ³buje predyktowaÄ‡ ERRORSâ‚!)     â”‚
â”‚                                      â”‚
â”‚     Predictionsâ‚‚ = [0.1, -0.2, ...] â”‚
â”‚     Errorsâ‚‚ = Errorsâ‚ - Predictionsâ‚‚â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 3: Powtarzaj dla N drzew     â”‚
â”‚                                      â”‚
â”‚  Final = Î£ (learning_rate Ã— Tree_i) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Gradient Boosting:**

- KaÅ¼de nowe drzewo "naprawia" bÅ‚Ä™dy poprzednich
- Gradient descent w przestrzeni funkcji!
- Learning rate kontroluje jak "mocno" poprawiamy

**Zalety:**

- âœ… Stan-of-the-art performance
- âœ… ObsÅ‚uguje missing values
- âœ… Wbudowana regularyzacja
- âœ… Szybki (parallelizacja)
- âœ… Feature importance

**Wady:**

- âŒ DuÅ¼o hiperparametrÃ³w do tuningu
- âŒ MoÅ¼e overfittowaÄ‡ bez early stopping
- âŒ Trudny do interpretacji
- âŒ Potrzebuje dobrego tuningu

**Hiperparametry:**

- `n_estimators`: liczba drzew (wiÄ™cej = lepiej, ale wolniej)
- `max_depth`: gÅ‚Ä™bokoÅ›Ä‡ drzew (mniej = mniej overfitting)
- `learning_rate`: jak "mocno" uczymy (mniej = bezpieczniej)
- `subsample`: % sampli na drzewo (80% = wiÄ™cej diversity)

**Kiedy uÅ¼yÄ‡:**

- Konkursy ML (Kaggle)
- Tabularne dane (cechy numeryczne)
- Masz czas na tuning
- Chcesz najlepszej accuracy

**W naszym projekcie:**

- 100 drzew, max_depth=6
- learning_rate=0.1
- subsample=0.8

---

### 9. âš¡ LightGBM (Light Gradient Boosting Machine)

**Jak dziaÅ‚a:**

```
Similar do XGBoost, ale z optymalizacjami:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Leaf-wise growth (not level)    â”‚
â”‚                                      â”‚
â”‚     XGBoost:        LightGBM:       â”‚
â”‚        â–²               â–²            â”‚
â”‚       / \             / \           â”‚
â”‚      /   \           /   \          â”‚
â”‚     /     \         /     \         â”‚
â”‚    (rÃ³wno) (gÅ‚Ä™biej tam gdzie gain) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Histogram-based splits          â”‚
â”‚  (grupuje cechy â†’ szybsze)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Efekt: 10-20x szybszy od XGBoost! â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Zalety:**

- âœ… BARDZO szybki
- âœ… MaÅ‚o pamiÄ™ci
- âœ… Podobna accuracy do XGBoost
- âœ… Dobry dla duÅ¼ych datasets

**Wady:**

- âŒ MoÅ¼e overfittowaÄ‡ na maÅ‚ych danych
- âŒ WraÅ¼liwy na parametry

**Kiedy uÅ¼yÄ‡:**

- DuÅ¼y dataset (>10K sampli)
- Potrzebujesz szybkoÅ›ci
- Tabularne dane

**W naszym projekcie:**

- Podobne parametry do XGBoost
- num_leaves=31 (max liÅ›ci)

---

## Modele Hybrydowe

### 10. âœï¸ Style-based Model

**Jak dziaÅ‚a:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 1: Extract style features     â”‚
â”‚  from text                          â”‚
â”‚                                      â”‚
â”‚  â€¢ avg_sentence_length              â”‚
â”‚  â€¢ avg_word_length                  â”‚
â”‚  â€¢ vocabulary_richness              â”‚
â”‚  â€¢ dialogue_ratio                   â”‚
â”‚  â€¢ punctuation_patterns             â”‚
â”‚  â€¢ capitalization_rate              â”‚
â”‚  â€¢ ...26 features total             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 2: Train Random Forest        â”‚
â”‚  on style features                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Intuicja:**

- RÃ³Å¼ne gatunki majÄ… rÃ³Å¼ny styl pisania
- Romance: krÃ³tsze zdania, wiÄ™cej wykrzyknikÃ³w, wiÄ™cej dialogÃ³w
- SciFi: dÅ‚uÅ¼sze sÅ‚owa, bardziej technical vocab
- Biography: konkretne daty, imiona, fakty

**Zalety:**

- âœ… Nie potrzebuje duÅ¼ego sÅ‚ownika
- âœ… Szybki (tylko 26 cech)
- âœ… Interpretowalne cechy

**Wady:**

- âŒ Gubi semantykÄ™ (treÅ›Ä‡)
- âŒ SÅ‚abszy niÅ¼ TF-IDF models

**Kiedy uÅ¼yÄ‡:**

- Jako dodatkowy model w ensemble
- Analiza stylu autora
- Gdy semantyka nie wystarcza

---

### 11. ğŸ”‘ Baseline Keyword Model

**Jak dziaÅ‚a:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 1: Define keywords per genre  â”‚
â”‚                                      â”‚
â”‚  Romance: ["love", "kiss", "heart"] â”‚
â”‚  Mystery: ["murder", "detective"]   â”‚
â”‚  SciFi: ["space", "alien", "robot"] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 2: Count keyword occurrences  â”‚
â”‚                                      â”‚
â”‚  Text: "She loved the detective..."â”‚
â”‚  Romance_score = 1 (love)           â”‚
â”‚  Mystery_score = 1 (detective)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KROK 3: Logistic Regression on    â”‚
â”‚  keyword counts                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Zalety:**

- âœ… Bardzo prosty
- âœ… Interpretowalny
- âœ… Szybki
- âœ… Dobry baseline

**Wady:**

- âŒ Wymaga rÄ™cznego wyboru keywords
- âŒ Nie uczy siÄ™ automatycznie
- âŒ SÅ‚abszy od TF-IDF

---

## Ensemble Models

### 12. ğŸ¤ Ensemble Voting

**Soft Voting:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model 1 (SVM):                     â”‚
â”‚    Romance: 0.7, Mystery: 0.2, ...  â”‚
â”‚                                      â”‚
â”‚  Model 2 (LogReg):                  â”‚
â”‚    Romance: 0.6, Mystery: 0.3, ...  â”‚
â”‚                                      â”‚
â”‚  Model 3 (RF):                      â”‚
â”‚    Romance: 0.8, Mystery: 0.1, ...  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Average probabilities:             â”‚
â”‚    Romance: (0.7+0.6+0.8)/3 = 0.70 â”‚
â”‚    Mystery: (0.2+0.3+0.1)/3 = 0.20 â”‚
â”‚  â†’ Prediction: Romance              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Hard Voting:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model 1: Romance                   â”‚
â”‚  Model 2: Romance                   â”‚
â”‚  Model 3: Mystery                   â”‚
â”‚  Model 4: Romance                   â”‚
â”‚  Model 5: Romance                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Majority vote: Romance (4/5)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Weighted Voting:**

```
Model 1 (best): weight = 0.5
Model 2: weight = 0.3
Model 3: weight = 0.2

Final = 0.5Ã—Predâ‚ + 0.3Ã—Predâ‚‚ + 0.2Ã—Predâ‚ƒ
```

**Zalety:**

- âœ… Prawie zawsze lepszy niÅ¼ single model
- âœ… Bardziej stabilny (robust)
- âœ… ÅÄ…czy rÃ³Å¼ne "spojrzenia" na problem

**Wady:**

- âŒ Wolniejszy (N modeli)
- âŒ WiÄ™cej pamiÄ™ci

**Kiedy uÅ¼yÄ‡:**

- Production (najwaÅ¼niejsza accuracy)
- RÃ³Å¼norodne modele (SVM + RF + NB lepsze niÅ¼ 3Ã— SVM)
- Masz zasoby obliczeniowe

---

## PorÃ³wnanie AlgorytmÃ³w

### Performance vs Complexity

```
High Performance â†‘
                 â”‚
            â­SVMâ”‚                  ğŸŒ³XGBoost
                 â”‚         âš¡LightGBM
                 â”‚    ğŸ“ŠLogReg
                 â”‚               ğŸŒ²RF
                 â”‚   ğŸ²NB
                 â”‚      ğŸ‘¥KNN
                 â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
                   Simple        Complex
```

### Training Time

```
Fast  â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  Slow
      â”‚                          â”‚
   ğŸ²NB  ğŸ¯Centroid  ğŸ“ŠLogReg  ğŸ“Ridge  â­SVM  ğŸ‘¥KNN  ğŸŒ²RF  ğŸŒ³XGBoost
```

### Interpretability

```
Easy to Understand  â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  Black Box
                    â”‚            â”‚
    ğŸ²NB  ğŸ”‘Keywords  ğŸ“ŠLogReg  ğŸ“Ridge  â­SVM  ğŸŒ²RF  ğŸŒ³XGBoost  âš¡LightGBM
```

### Memory Usage

```
Low Memory  â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  High Memory
            â”‚                 â”‚
  ğŸ¯Centroid  â­SVM  ğŸ“ŠLogReg  ğŸ²NB  ğŸ“Ridge  ğŸ‘¥KNN  ğŸŒ²RF  ğŸŒ³XGBoost
```

---

## Kiedy KtÃ³rego UÅ¼yÄ‡

### ğŸ¯ ChcÄ™ najlepszej accuracy (competition)

1. **XGBoost** / **LightGBM** - extensive tuning
2. **Ensemble** z top 3-5 modeli
3. **Linear SVM** - jeÅ›li text-based

### âš¡ PotrzebujÄ™ szybkoÅ›ci (production)

1. **Nearest Centroid** - najszybszy
2. **Naive Bayes** - bardzo szybki
3. **Linear SVM** - szybkie predykcje

### ğŸ” ChcÄ™ interpretowalnoÅ›ci

1. **Logistic Regression** - wagi dla kaÅ¼dego sÅ‚owa
2. **Naive Bayes** - prawdopodobieÅ„stwa sÅ‚Ã³w
3. **Decision Trees** - Å›cieÅ¼ka decyzyjna

### ğŸ“¦ MaÅ‚y dataset (<1000 sampli)

1. **Naive Bayes** - dziaÅ‚a na maÅ‚ych danych
2. **Logistic Regression** z regularyzacjÄ…
3. **KNN** - no training needed

### ğŸ¨ RÃ³Å¼norodne typy cech

1. **Random Forest** - nie potrzebuje skalowania
2. **XGBoost** - obsÅ‚uguje rÃ³Å¼ne typy
3. **LightGBM** - szybki na mixed features

### ğŸ Pierwszy model (baseline)

1. **Logistic Regression** - zawsze zacznij tutaj
2. **Naive Bayes** - szybki baseline
3. **Linear SVM** - jeÅ›li LogReg nie wystarcza

---

## Praktyczne WskazÃ³wki

### ğŸ“ Workflow dla Nowego Projektu

```
KROK 1: Quick baselines (1 dzieÅ„)
  â”œâ”€ Naive Bayes
  â”œâ”€ Logistic Regression
  â””â”€ See what accuracy is possible

KROK 2: Try stronger models (2-3 dni)
  â”œâ”€ Linear SVM
  â”œâ”€ Random Forest
  â””â”€ XGBoost

KROK 3: Hyperparameter tuning (3-5 dni)
  â”œâ”€ Grid search na top 2-3 modelach
  â””â”€ Cross-validation

KROK 4: Ensemble (1 dzieÅ„)
  â””â”€ Combine best models

KROK 5: Production optimization
  â”œâ”€ Speed vs accuracy tradeoff
  â””â”€ Deploy simplest model that meets requirements
```

### âš ï¸ CzÄ™ste BÅ‚Ä™dy

1. **Zaczynanie od XGBoost**

   - âŒ ZÅ‚e: "XGBoost jest najlepszy, zacznÄ™ od niego"
   - âœ… Dobre: Zacznij od prostych modeli â†’ zrozum dane â†’ potem XGBoost

2. **Nie sprawdzanie overfittingu**

   - âŒ ZÅ‚e: "100% train accuracy! Super!"
   - âœ… Dobre: Zawsze porÃ³wnaj train vs test accuracy

3. **Ignorowanie baseline**

   - âŒ ZÅ‚e: Pomijanie Logistic Regression
   - âœ… Dobre: LogReg mÃ³wi Ci czy problem jest Å‚atwy czy trudny

4. **ZÅ‚e metryki**

   - âŒ ZÅ‚e: Patrzenie tylko na accuracy (niezbalansowane klasy)
   - âœ… Dobre: F1 score, confusion matrix, per-class metrics

5. **Nie testowanie na nowych danych**
   - âŒ ZÅ‚e: Test na tym samym zbiorze
   - âœ… Dobre: Hold-out test set OR cross-validation

---

## Podsumowanie

### Top 3 dla Text Classification:

1. **ğŸ¥‡ Linear SVM** - best accuracy, fast predictions
2. **ğŸ¥ˆ Logistic Regression** - probabilites, interpretable
3. **ğŸ¥‰ XGBoost** - with feature engineering

### Top 3 dla Quick Prototyping:

1. **ğŸ¥‡ Naive Bayes** - fastest
2. **ğŸ¥ˆ Logistic Regression** - good baseline
3. **ğŸ¥‰ Nearest Centroid** - simple and fast

### Top 3 dla Production:

1. **ğŸ¥‡ Ensemble** (SVM + LogReg + XGBoost)
2. **ğŸ¥ˆ Linear SVM** - single model
3. **ğŸ¥‰ Logistic Regression** - interpretable

---

## Dodatkowe Å¹rÃ³dÅ‚a

### KsiÄ…Å¼ki:

- "Hands-On Machine Learning" - AurÃ©lien GÃ©ron
- "Pattern Recognition and Machine Learning" - Christopher Bishop
- "The Elements of Statistical Learning" - Hastie, Tibshirani, Friedman

### Kursy Online:

- Andrew Ng - Machine Learning (Coursera)
- Fast.ai - Practical Deep Learning
- Scikit-learn documentation

### Papers:

- SVM: "Support Vector Networks" (Cortes & Vapnik, 1995)
- XGBoost: "XGBoost: A Scalable Tree Boosting System" (Chen & Guestrin, 2016)
- Random Forest: "Random Forests" (Breiman, 2001)

---

**Pytania? Sugestie? Issues?**
https://github.com/your-username/book-genre-classifier

**Autor:** Kasia
**Ostatnia aktualizacja:** 2026-02-09
